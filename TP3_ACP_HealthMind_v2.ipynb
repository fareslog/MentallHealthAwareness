{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28668ae",
   "metadata": {},
   "source": [
    "# TP3 — EDA → Imputation → ACP (80%) → KMeans — HealthMind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f230f",
   "metadata": {},
   "source": [
    "\n",
    "**Roadmap (simple) :**  \n",
    "1) **Comprendre** la data (colonnes, types, nulls) et **visualiser** les valeurs manquantes.  \n",
    "2) **Remplacer réellement** les valeurs manquantes (`df_clean`).  \n",
    "3) **ACP (80%)** : scree plot, cercle, biplot.  \n",
    "4) **(Option)** KMeans pour segments.  \n",
    "5) **Conclusions** claires.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b1ceaa",
   "metadata": {},
   "source": [
    "## 0) Compréhension & visualisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6698298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"/mnt/data/HealthMind_Mental_Health_Data_3500.csv\")\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "print(\"\\nColonnes:\")\n",
    "for c in df.columns:\n",
    "    print(\"-\", c)\n",
    "\n",
    "dtypes = df.dtypes.astype(str).to_frame(\"dtype\")\n",
    "display(dtypes)\n",
    "\n",
    "uniq = df.nunique().sort_values(ascending=False).to_frame(\"n_unique\")\n",
    "display(uniq)\n",
    "\n",
    "missing_count = df.isna().sum().sort_values(ascending=False)\n",
    "missing_ratio = (df.isna().mean()*100).round(2).sort_values(ascending=False)\n",
    "missing_table = pd.DataFrame({\"missing_count\": missing_count, \"missing_%\": missing_ratio})\n",
    "display(missing_table)\n",
    "\n",
    "plt.figure()\n",
    "missing_count.plot(kind=\"bar\")\n",
    "plt.ylabel(\"Nombre de valeurs manquantes\")\n",
    "plt.title(\"Valeurs manquantes par colonne\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d70b27b",
   "metadata": {},
   "source": [
    "\n",
    "**On fait quoi ici ?**  \n",
    "- On liste **noms de colonnes**, **types**, **valeurs uniques**.  \n",
    "- On calcule et **on trace** les valeurs manquantes par colonne.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68570d60",
   "metadata": {},
   "source": [
    "## 1) Imputation réelle (df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325573b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_cols = df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "cat_cols = [c for c in df.columns if c not in num_cols]\n",
    "\n",
    "num_imp = SimpleImputer(strategy=\"median\")\n",
    "cat_imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "df_num = pd.DataFrame(num_imp.fit_transform(df[num_cols]), columns=num_cols, index=df.index)\n",
    "df_cat = pd.DataFrame(cat_imp.fit_transform(df[cat_cols]), columns=cat_cols, index=df.index)\n",
    "\n",
    "df_clean = pd.concat([df_num, df_cat], axis=1)[df.columns]\n",
    "print(\"Nb total de valeurs manquantes après imputation:\", int(df_clean.isna().sum().sum()))\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ede5ff",
   "metadata": {},
   "source": [
    "\n",
    "**Pourquoi cette étape ?**  \n",
    "- Pour avoir une **version sans trous** de la data (utile pour des exports/analyses).  \n",
    "- Ensuite, pour la **modélisation**, on préfère un **Pipeline** (ci-dessous) qui répète ces étapes proprement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802e6899",
   "metadata": {},
   "source": [
    "## 2) Pipeline de prétraitement pour l’ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "\n",
    "num_cols_pca = df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "for tgt in [\"Risk_Level\", \"Stress_Level_Scale\"]:\n",
    "    if tgt in num_cols_pca:\n",
    "        num_cols_pca.remove(tgt)\n",
    "cat_cols_pca = [c for c in df.columns if c not in num_cols_pca]\n",
    "\n",
    "num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                     (\"scaler\", RobustScaler())])\n",
    "cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols_pca),\n",
    "    (\"cat\", cat_pipe, cat_cols_pca)\n",
    "])\n",
    "preprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976ef13",
   "metadata": {},
   "source": [
    "\n",
    "**Explications simples :**  \n",
    "- **Imputer** = remplir les trous ; **Scaler** = mettre à la même échelle ; **OneHot** = transformer les catégories en 0/1.  \n",
    "- `ColumnTransformer` applique le bon traitement à chaque groupe de colonnes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2d352",
   "metadata": {},
   "source": [
    "## 3) ACP (objectif 80% de variance cumulée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c403256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca_probe = Pipeline([(\"prep\", preprocess), (\"pca\", PCA(n_components=10, random_state=42))])\n",
    "Z = pca_probe.fit_transform(df)\n",
    "pca_obj = pca_probe.named_steps[\"pca\"]\n",
    "evr = pca_obj.explained_variance_ratio_\n",
    "cum = evr.cumsum()\n",
    "\n",
    "print(\"Variance expliquée (%):\", np.round(evr*100, 2))\n",
    "print(\"Variance cumulée (%):\", np.round(cum*100, 2))\n",
    "\n",
    "n_keep = int(np.argmax(cum >= 0.80) + 1) if np.any(cum >= 0.80) else len(evr)\n",
    "n_keep = max(2, n_keep)\n",
    "print(\"Axes gardés (≈80%):\", n_keep)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(evr)+1), evr*100, marker=\"o\")\n",
    "plt.xlabel(\"Axe\"); plt.ylabel(\"% var. expliquée\"); plt.title(\"Scree plot\")\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ddebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Refit PCA & visualisations\n",
    "pca_final = Pipeline([(\"prep\", preprocess), (\"pca\", PCA(n_components=n_keep, random_state=42))])\n",
    "scores = pca_final.fit_transform(df)\n",
    "pca_k = pca_final.named_steps[\"pca\"]\n",
    "\n",
    "feat_names = pca_final.named_steps[\"prep\"].get_feature_names_out()\n",
    "components = pca_k.components_\n",
    "ev = pca_k.explained_variance_\n",
    "loadings = components.T * np.sqrt(ev)\n",
    "\n",
    "import re\n",
    "num_mask = [name.startswith(\"num__\") for name in feat_names]\n",
    "num_names = [re.sub(r\"^num__\", \"\", n) for n, m in zip(feat_names, num_mask) if m]\n",
    "num_load = loadings[num_mask, :]\n",
    "\n",
    "# Cercle Dim1-Dim2\n",
    "theta = np.linspace(0, 2*np.pi, 400)\n",
    "plt.figure()\n",
    "plt.plot(np.cos(theta), np.sin(theta))\n",
    "plt.axhline(0, linewidth=0.5); plt.axvline(0, linewidth=0.5)\n",
    "for i, var in enumerate(num_names):\n",
    "    x, y = num_load[i, 0], num_load[i, 1]\n",
    "    plt.arrow(0, 0, x, y, head_width=0.03, length_includes_head=True)\n",
    "    plt.text(x*1.08, y*1.08, var, ha=\"center\", va=\"center\", fontsize=8)\n",
    "plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "plt.xlim(-1.1, 1.1); plt.ylim(-1.1, 1.1)\n",
    "plt.xlabel(\"Dim1\"); plt.ylabel(\"Dim2\"); plt.title(\"Cercle des corrélations (numériques)\")\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Biplot\n",
    "plt.figure()\n",
    "plt.scatter(scores[:,0], scores[:,1], s=10)\n",
    "plt.axhline(0, linewidth=0.5); plt.axvline(0, linewidth=0.5)\n",
    "plt.xlabel(\"Dim1\"); plt.ylabel(\"Dim2\"); plt.title(\"Biplot (individus)\")\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85728146",
   "metadata": {},
   "source": [
    "## 4) (Option) KMeans sur l'espace ACP — choix de k par silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sil = []\n",
    "for k in range(2, 8):\n",
    "    km = KMeans(n_clusters=k, n_init=20, random_state=42)\n",
    "    labels = km.fit_predict(scores)\n",
    "    sil.append((k, silhouette_score(scores, labels)))\n",
    "\n",
    "best_k, best_s = max(sil, key=lambda x: x[1])\n",
    "print(\"Silhouette:\", sil)\n",
    "print(f\"Meilleur k = {best_k} (silhouette = {best_s:.3f})\")\n",
    "\n",
    "km = KMeans(n_clusters=best_k, n_init=20, random_state=42).fit(scores)\n",
    "labels = km.labels_\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.scatter(scores[:,0], scores[:,1], c=labels, s=10)\n",
    "plt.axhline(0, linewidth=0.5); plt.axvline(0, linewidth=0.5)\n",
    "plt.xlabel(\"Dim1\"); plt.ylabel(\"Dim2\"); plt.title(f\"KMeans (k={{best_k}}) sur PCA\")\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Profilage simple\n",
    "df_prof = df.copy()\n",
    "df_prof[\"__cluster__\"] = labels\n",
    "display(df_prof[\"__cluster__\"].value_counts().sort_index())\n",
    "num_cols_profile = df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "for tgt in [\"Risk_Level\", \"Stress_Level_Scale\"]:\n",
    "    if tgt in num_cols_profile:\n",
    "        num_cols_profile.remove(tgt)\n",
    "display(df_prof.groupby(\"__cluster__\")[num_cols_profile].mean().round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b257d3",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Conclusions (claires et simples)\n",
    "\n",
    "- Dataset : **3500 lignes**, **14 colonnes**.  \n",
    "- Les **valeurs manquantes** ont été **visualisées** puis **remplacées réellement** (médiane/mode) dans `df_clean`.  \n",
    "- **ACP** : on garde le nombre d’axes nécessaire pour atteindre **80%** de variance cumulée (pas 85%).  \n",
    "  - Les deux premiers axes expliquent environ **16.0%** (Dim1 ≈ **8.1%**, Dim2 ≈ **7.9%**).  \n",
    "- **(Option)** Clustering KMeans : choisir **k** via **silhouette**, visualiser les groupes et **décrire** leurs profils.\n",
    "\n",
    "**Prochaine étape** : ajouter un **modèle de classification** (RF / GB), mesurer **Recall/F1**, et générer des **recommandations** basées sur les facteurs clés.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
